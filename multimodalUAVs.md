Multimodal image fusion for coastal ecosystem mapping with UAVs
--
Advisor: Joseph Garrett

The coastal environment contains many ecologically important species such as kelp and seagrass. Monitoring and mapping the coast can be a tool for ensuring the health and prosperity of these species. For that reason, [NIVA](https://www.niva.no/), in collaboration with NTNU, is developing a system for mapping the coast with drones. Several different imagers are carried by different drones, including an rgb camera, a multispectral camera, and a hyperspectral camera. The hyperspectral camera measures the most information, while the rgb camera can image the largest area with the highest resolution. In the project, we would like to fuse the information collected by the sensors in order to combine the advantages of the different imaging modalities. 

The focus of this project will be on developing a simple multimodal image fusion technique which can then be applied to the data that the SeaBee project has already collected (100s of GB) so that its uncertainty and reliability can be characterized. One possible technique for the image fusion could be [self-organizing maps, a type of shallow neural network](https://www.mdpi.com/2072-4292/12/1/7), but other techniques are welcome if you would prefer to try something else. The advantage of SOMs is that they are simple, quite reliable, and the relevant uncertainties can be visualized straightforwardly. 

As this project is associated with NIVA and the [SeaBee consortium](https://seabee.no/), it will be possible to access the Sigma2 supercomputer. Use of the supercomputer should be fairly simple, and the folks at NIVA assure me that simple computing environments, such as Jupyter, can be run on Sigma2. In addition, funds are available to support a drone mapping expedition to the Norwegian coast, if more data are desired. Moreover, there may be imaging missions during the semester which you could join. 

Please contact Joe at joseph.garrett@ntnu.no for more information. 
